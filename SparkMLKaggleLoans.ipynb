{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Trabalho - Spark ML - V2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGTT0RcDCv5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "889e6a25-f4f3-48ce-a8bb-61abf41ff01f"
      },
      "source": [
        "!pip install pyspark\n",
        "!pip install spark-sklearn\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkFiles\n",
        "spark_session = SparkSession.builder.master('local[*]').appName('sparkML').getOrCreate()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/26/198fc8c0b98580f617cb03cb298c6056587b8f0447e20fa40c5b634ced77/pyspark-3.0.1.tar.gz (204.2MB)\n",
            "\u001b[K     |████████████████████████████████| 204.2MB 60kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 44.9MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612243 sha256=2316391971d734c4ee286a33ca6a649e94879f2a0456b30c4073fa2de58d9dfa\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/bd/07/031766ca628adec8435bb40f0bd83bb676ce65ff4007f8e73f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.0.1\n",
            "Collecting spark-sklearn\n",
            "  Downloading https://files.pythonhosted.org/packages/b0/3f/34b8dec7d2cfcfe0ba99d637b4f2d306c1ca0b404107c07c829e085f6b38/spark-sklearn-0.3.0.tar.gz\n",
            "Collecting scikit-learn<0.20,>=0.18.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/c8/8db4108aba5e2166cd2ea4eafa1a4b82f89240a1fa85733029cc2358ad1f/scikit_learn-0.19.2-cp36-cp36m-manylinux1_x86_64.whl (4.9MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9MB 7.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: spark-sklearn\n",
            "  Building wheel for spark-sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for spark-sklearn: filename=spark_sklearn-0.3.0-cp36-none-any.whl size=30592 sha256=ea3a1edb6263765ffcf2be37e01aa34d85709f0004b7185a8a1797c53ff79a3b\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/28/e8/cb0250888675c630786f932dcc63ed96ac1aca299bcfb7235f\n",
            "Successfully built spark-sklearn\n",
            "\u001b[31mERROR: yellowbrick 0.9.1 has requirement scikit-learn>=0.20, but you'll have scikit-learn 0.19.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: umap-learn 0.4.6 has requirement scikit-learn>=0.20, but you'll have scikit-learn 0.19.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: imbalanced-learn 0.4.3 has requirement scikit-learn>=0.20, but you'll have scikit-learn 0.19.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: scikit-learn, spark-sklearn\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed scikit-learn-0.19.2 spark-sklearn-0.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwyCYg-PHHOI"
      },
      "source": [
        "url = 'http://www.ppgia.pucpr.br/~jean.barddal/bigdata/sparkml/giveMeLoanKaggle.csv'\n",
        "spark_session.sparkContext.addFile(url)\n",
        "df = spark_session.read.csv(\"file://\"+SparkFiles.get(\"giveMeLoanKaggle.csv\"), header=True, inferSchema= True, nullValue='?')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dggQDVHnHH9L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "175ecccf-1c84-46f0-dce2-9192077ff0b2"
      },
      "source": [
        "df.printSchema()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- RevolvingUtilizationOfUnsecuredLines: double (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- NumberOfTime30-59DaysPastDueNotWorse: integer (nullable = true)\n",
            " |-- DebtRatio: double (nullable = true)\n",
            " |-- MonthlyIncome: integer (nullable = true)\n",
            " |-- NumberOfOpenCreditLinesAndLoans: integer (nullable = true)\n",
            " |-- NumberOfTimes90DaysLate: integer (nullable = true)\n",
            " |-- NumberRealEstateLoansOrLines: integer (nullable = true)\n",
            " |-- NumberOfTime60-89DaysPastDueNotWorse: integer (nullable = true)\n",
            " |-- NumberOfDependents: integer (nullable = true)\n",
            " |-- isFraud: integer (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7en3QRO-HPIG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "862433d9-ccad-4849-d07a-3f0d4d261efb"
      },
      "source": [
        "from pyspark.sql.functions import col\n",
        "df.groupBy('NumberOfDependents').count().orderBy(col('count').desc()).show()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------+-----+\n",
            "|NumberOfDependents|count|\n",
            "+------------------+-----+\n",
            "|                 0|86903|\n",
            "|                 1|26316|\n",
            "|                 2|19522|\n",
            "|                 3| 9483|\n",
            "|              null| 3923|\n",
            "|                 4| 2862|\n",
            "|                 5|  746|\n",
            "|                 6|  158|\n",
            "|                 7|   51|\n",
            "|                 8|   24|\n",
            "|                 9|    5|\n",
            "|                10|    5|\n",
            "|                13|    1|\n",
            "|                20|    1|\n",
            "+------------------+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjWGYVJKNsDi"
      },
      "source": [
        "from pyspark.ml.feature import Imputer\n",
        "imputer = Imputer(strategy='mean', inputCols=(['MonthlyIncome','NumberOfDependents']), outputCols=(['OUT_MonthlyIncome','OUT_NumberOfDependents']) )"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "va1R_S4RR85s"
      },
      "source": [
        "model = imputer.fit(df)\n",
        "df_normalizado=model.transform(df)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsHu_f6aTHgW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1af5816-bfbf-4bf4-959d-a9099ba2766e"
      },
      "source": [
        "df_normalizado = df_normalizado[['RevolvingUtilizationOfUnsecuredLines','age','NumberOfTime30-59DaysPastDueNotWorse','DebtRatio','NumberOfOpenCreditLinesAndLoans','NumberOfTimes90DaysLate','NumberRealEstateLoansOrLines','NumberOfTime60-89DaysPastDueNotWorse','OUT_MonthlyIncome','OUT_NumberOfDependents','isFraud']]\n",
        "df_normalizado.show(5)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------------------------+---+------------------------------------+---------+-------------------------------+-----------------------+----------------------------+------------------------------------+-----------------+----------------------+-------+\n",
            "|RevolvingUtilizationOfUnsecuredLines|age|NumberOfTime30-59DaysPastDueNotWorse|DebtRatio|NumberOfOpenCreditLinesAndLoans|NumberOfTimes90DaysLate|NumberRealEstateLoansOrLines|NumberOfTime60-89DaysPastDueNotWorse|OUT_MonthlyIncome|OUT_NumberOfDependents|isFraud|\n",
            "+------------------------------------+---+------------------------------------+---------+-------------------------------+-----------------------+----------------------------+------------------------------------+-----------------+----------------------+-------+\n",
            "|                            0.766127| 45|                                   2| 0.802982|                             13|                      0|                           6|                                   0|             9120|                     2|      1|\n",
            "|                            0.957151| 40|                                   0| 0.121876|                              4|                      0|                           0|                                   0|             2600|                     1|      0|\n",
            "|                             0.65818| 38|                                   1| 0.085113|                              2|                      1|                           0|                                   0|             3042|                     0|      0|\n",
            "|                             0.23381| 30|                                   0|  0.03605|                              5|                      0|                           0|                                   0|             3300|                     0|      0|\n",
            "|                            0.907239| 49|                                   1| 0.024926|                              7|                      0|                           1|                                   0|            63588|                     0|      0|\n",
            "+------------------------------------+---+------------------------------------+---------+-------------------------------+-----------------------+----------------------------+------------------------------------+-----------------+----------------------+-------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6esbLduy7c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb6b3a12-6f86-4dd5-f5c0-566512c201ec"
      },
      "source": [
        "cols_treinamento = list(df_normalizado.columns)\n",
        "cols_treinamento.remove('isFraud')\n",
        "print(cols_treinamento)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['RevolvingUtilizationOfUnsecuredLines', 'age', 'NumberOfTime30-59DaysPastDueNotWorse', 'DebtRatio', 'NumberOfOpenCreditLinesAndLoans', 'NumberOfTimes90DaysLate', 'NumberRealEstateLoansOrLines', 'NumberOfTime60-89DaysPastDueNotWorse', 'OUT_MonthlyIncome', 'OUT_NumberOfDependents']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOXuMMWA12qj"
      },
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "assembler = VectorAssembler(inputCols=cols_treinamento, outputCol='features_treinamento')\n",
        "df_normalizado = assembler.transform(df_normalizado)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOosTv9f2KNT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "816c1692-ec9b-4639-f280-87a7439a96f0"
      },
      "source": [
        "df_normalizado.show(5)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------------------------+---+------------------------------------+---------+-------------------------------+-----------------------+----------------------------+------------------------------------+-----------------+----------------------+-------+--------------------+\n",
            "|RevolvingUtilizationOfUnsecuredLines|age|NumberOfTime30-59DaysPastDueNotWorse|DebtRatio|NumberOfOpenCreditLinesAndLoans|NumberOfTimes90DaysLate|NumberRealEstateLoansOrLines|NumberOfTime60-89DaysPastDueNotWorse|OUT_MonthlyIncome|OUT_NumberOfDependents|isFraud|features_treinamento|\n",
            "+------------------------------------+---+------------------------------------+---------+-------------------------------+-----------------------+----------------------------+------------------------------------+-----------------+----------------------+-------+--------------------+\n",
            "|                            0.766127| 45|                                   2| 0.802982|                             13|                      0|                           6|                                   0|             9120|                     2|      1|[0.766127,45.0,2....|\n",
            "|                            0.957151| 40|                                   0| 0.121876|                              4|                      0|                           0|                                   0|             2600|                     1|      0|[0.957151,40.0,0....|\n",
            "|                             0.65818| 38|                                   1| 0.085113|                              2|                      1|                           0|                                   0|             3042|                     0|      0|[0.65818,38.0,1.0...|\n",
            "|                             0.23381| 30|                                   0|  0.03605|                              5|                      0|                           0|                                   0|             3300|                     0|      0|(10,[0,1,3,4,8],[...|\n",
            "|                            0.907239| 49|                                   1| 0.024926|                              7|                      0|                           1|                                   0|            63588|                     0|      0|[0.907239,49.0,1....|\n",
            "+------------------------------------+---+------------------------------------+---------+-------------------------------+-----------------------+----------------------------+------------------------------------+-----------------+----------------------+-------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AF0IoMkX4bur"
      },
      "source": [
        "# Logistic Regeression (Holdout - 10 Folds)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BfArCLPwAAx"
      },
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "\n",
        "train, test = df_normalizado.randomSplit([0.7,0.3], seed=10)\n",
        "lr = LogisticRegression(featuresCol='features_treinamento', labelCol='isFraud')\n",
        "pipeline = Pipeline(stages=[lr])\n",
        "paramGrid = ParamGridBuilder()\\\n",
        "    .addGrid(lr.regParam, [0.1, 0.01])\\\n",
        "    .build()\n",
        "\n",
        "\n",
        "crossval = CrossValidator(estimator=pipeline,\n",
        "                          estimatorParamMaps=paramGrid,\n",
        "                          evaluator=BinaryClassificationEvaluator(labelCol=\"isFraud\"),\n",
        "                          numFolds=10) \n",
        "\n",
        "cvModel = crossval.fit(train)\n",
        "\n",
        "\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Bo7Bzz3lXx1"
      },
      "source": [
        "# Logistic Regeression Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kF3jFNNNdNA",
        "outputId": "0655963f-7352-4757-cf69-de32dae4ba0e"
      },
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "\n",
        "avaliador = MulticlassClassificationEvaluator(labelCol='isFraud', predictionCol='prediction', metricName='accuracy')\n",
        "\n",
        "predicoes_lr = cvModel.transform(test)\n",
        "predicoes_lr = predicoes_lr.withColumn('isFraud', predicoes_lr ['isFraud'].cast('double'))\n",
        "predictions_and_labels = predicoes_lr.rdd.map(lambda lp: (lp['prediction'], lp['isFraud']))\n",
        "\n",
        "calculadora = MulticlassMetrics(predictions_and_labels)\n",
        "\n",
        "\n",
        "print(f'Tx de acerto = {100.0*calculadora.accuracy}%')\n",
        "print(f'Recall ponderado = {100.0*calculadora.weightedRecall}%')\n",
        "print(f'Precisão ponderada = {100.0*calculadora.weightedPrecision}%')\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tx de acerto = 93.4097995545657%\n",
            "Recall ponderado = 93.4097995545657%\n",
            "Precisão ponderada = 90.99310663566538%\n",
            "[[4.1904e+04 2.9000e+01]\n",
            " [2.9300e+03 3.7000e+01]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkvTg6XfleFS"
      },
      "source": [
        "# Decision Tree Classifier (Holdout - 10 Folds)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YK7svmwVg3gk"
      },
      "source": [
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "\n",
        "dt = DecisionTreeClassifier(featuresCol='features_treinamento', labelCol='isFraud')\n",
        "pipeline = Pipeline(stages=[dt])\n",
        "paramGrid = ParamGridBuilder().build()\n",
        "     \n",
        "\n",
        "\n",
        "crossval = CrossValidator(estimator=pipeline,\n",
        "                          estimatorParamMaps=paramGrid,\n",
        "                          evaluator=BinaryClassificationEvaluator(labelCol=\"isFraud\"),\n",
        "                          numFolds=10) \n",
        "\n",
        "cvModel = crossval.fit(train)\n",
        "\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G02Sn7A5l95a"
      },
      "source": [
        "# Decision Tree Results\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwE7iTxflzc5"
      },
      "source": [
        "avaliador = MulticlassClassificationEvaluator(labelCol='isFraud', predictionCol='prediction', metricName='accuracy')\n",
        "\n",
        "predicoes_dt = cvModel.transform(test)\n",
        "predicoes_dt = predicoes_rf.withColumn('isFraud', predicoes_dt ['isFraud'].cast('double'))\n",
        "predictions_and_labels = predicoes_dt.rdd.map(lambda dt: (rf['prediction'], dt['isFraud']))\n",
        "\n",
        "calculadora = MulticlassMetrics(predictions_and_labels)\n",
        "\n",
        "\n",
        "print(f'Tx de acerto = {100.0*calculadora.accuracy}%')\n",
        "print(f'Recall ponderado = {100.0*calculadora.weightedRecall}%')\n",
        "print(f'Precisão ponderada = {100.0*calculadora.weightedPrecision}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayG8HZQqmJHI"
      },
      "source": [
        "# Random Forest Classifier (Holdout - 10 Folds)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5jJtMwFi962"
      },
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "rf = RandomForestClassifier(featuresCol='features_treinamento', labelCol='isFraud', numTrees=50)\n",
        "pipeline = Pipeline(stages=[rf])\n",
        "paramGrid = ParamGridBuilder().build()\n",
        "\n",
        "crossval = CrossValidator(estimator=pipeline,\n",
        "                          estimatorParamMaps=paramGrid,\n",
        "                          evaluator=BinaryClassificationEvaluator(labelCol=\"isFraud\"),\n",
        "                          numFolds=10) \n",
        "\n",
        "cvModel = crossval.fit(train)\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQ6GjDLcmTSR"
      },
      "source": [
        "# Random Forest Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOFVCtrP36F8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f8efd5c-022a-4d2f-b53a-00e3456fcb01"
      },
      "source": [
        "avaliador = MulticlassClassificationEvaluator(labelCol='isFraud', predictionCol='prediction', metricName='accuracy')\n",
        "\n",
        "predicoes_rf = cvModel.transform(test)\n",
        "predicoes_rf = predicoes_rf.withColumn('isFraud', predicoes_rf ['isFraud'].cast('double'))\n",
        "predictions_and_labels = predicoes_rf.rdd.map(lambda rf: (rf['prediction'], rf['isFraud']))\n",
        "\n",
        "calculadora = MulticlassMetrics(predictions_and_labels)\n",
        "\n",
        "\n",
        "print(f'Tx de acerto = {100.0*calculadora.accuracy}%')\n",
        "print(f'Recall ponderado = {100.0*calculadora.weightedRecall}%')\n",
        "print(f'Precisão ponderada = {100.0*calculadora.weightedPrecision}%')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tx de acerto = 93.63474387527839%\n",
            "Recall ponderado = 93.63474387527839%\n",
            "Precisão ponderada = 91.792161264034%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}